{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = \"results/all_vec.csv\"\n",
    "data_list = np.genfromtxt(dataset)\n",
    "data_list = data_list.astype(float)\n",
    "print(\"Dataset size: \", data_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "train_encoding = 10000 # number of packets to train packet feature encoding on\n",
    "train_clusters = 100000 # number of packets to train clustering module on\n",
    "W_seg = 50 # framing length \n",
    "C = 10 # adjustment in log transform\n",
    "K = 10 # number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature encodings using PCA\n",
    "train_encoding_data = data_list[:train_encoding]\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(train_encoding_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the remaining data using the learned PCA\n",
    "remaining_data = data_list[train_encoding:]\n",
    "embedded_data = pca.transform(remaining_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_packets = embedded_data.shape[0]\n",
    "n_frames = n_packets // W_seg\n",
    "print(\"Number of packets: \", n_packets)\n",
    "print(\"Number of frames: \", n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the modulus of DFT outputs\n",
    "modulus_dft = []\n",
    "\n",
    "# Perform DFT on each frame and calculate the modulus\n",
    "for i in range(n_frames):\n",
    "    frame = embedded_data[i*W_seg:(i+1)*W_seg]\n",
    "    dft_output = np.fft.fft(frame)\n",
    "    modulus_output = np.abs(dft_output)\n",
    "    modulus_dft.append(modulus_output)\n",
    "modulus_dft = np.array(modulus_dft)\n",
    "\n",
    "print(\"Modulus DFT shape: \", modulus_dft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transform to modulus of DFT outputs\n",
    "log_modulus_dft = np.log2(modulus_dft + np.ones(modulus_dft.shape))/C\n",
    "\n",
    "# Check for NaN and Inf values\n",
    "nan_mask = np.isnan(log_modulus_dft)\n",
    "inf_mask = np.isinf(log_modulus_dft)\n",
    "print(\"NaN values: \", np.sum(nan_mask))\n",
    "print(\"Inf values: \", np.sum(inf_mask))\n",
    "\n",
    "# Replace NaN and Inf values with 0\n",
    "log_modulus_dft = np.where(nan_mask | inf_mask, 0, log_modulus_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters = 100000 // W_seg\n",
    "train_data = np.mean(log_modulus_dft[:train_clusters], axis=2)\n",
    "test_data = np.mean(log_modulus_dft[train_clusters:], axis=2)\n",
    "all_data = np.mean(log_modulus_dft, axis=2)\n",
    "\n",
    "# Fit KMeans clustering model on training data\n",
    "kmeans = KMeans(n_clusters=20, n_init='auto')\n",
    "kmeans.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_distances(dataset, kmeans):\n",
    "    # Find the closest cluster center for each data point\n",
    "    closest_cluster_centers = kmeans.cluster_centers_[kmeans.predict(dataset)]\n",
    "\n",
    "    # Calculate the L2 distance between each data point and its closest cluster center\n",
    "    l2_distances = np.linalg.norm(dataset - closest_cluster_centers, axis=1)\n",
    "    print(\"L2 distances mean: \", np.mean(l2_distances))\n",
    "    print(\"L2 distances std: \", np.std(l2_distances))\n",
    "    return l2_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of l2 distances\n",
    "plt.hist(get_l2_distances(all_data, kmeans), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(get_l2_distances(test_data, kmeans), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold\n",
    "threshold = 1.4\n",
    "\n",
    "# Convert from frames back to packets\n",
    "num_data_pts = data_list.shape[0]\n",
    "l2_distances = get_l2_distances(all_data, kmeans)\n",
    "l2_dist_packet = np.append(np.zeros(train_encoding),np.repeat(l2_distances, W_seg))\n",
    "print(\"L2 distances shape: \", l2_dist_packet.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"mirai_labels.csv\"\n",
    "labels_list = np.genfromtxt(labels)\n",
    "labels_list = labels_list.astype(float)\n",
    "print(\"Labels shape: \", labels_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_dist = []\n",
    "normal_dist = []\n",
    "anomaly_indices = []\n",
    "for i in range(l2_dist_packet.shape[0]):\n",
    "    if labels_list[i] == 1:\n",
    "        anomaly_dist.append(l2_dist_packet[i])\n",
    "        anomaly_indices.append(i)\n",
    "    elif i > train_encoding:\n",
    "        normal_dist.append(l2_dist_packet[i])\n",
    "\n",
    "normal_dist = np.array(normal_dist)\n",
    "anomaly_dist = np.array(anomaly_dist)\n",
    "anomaly_indices = np.array(anomaly_indices)\n",
    "np.save(\"results/normal_dist.npy\", normal_dist)\n",
    "np.save(\"results/anomaly_dist.npy\", anomaly_dist)\n",
    "np.save(\"results/anomaly_indices.npy\", anomaly_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC of ROC curve\n",
    "def get_roc_auc(normal_rmses, anomaly_rmses):\n",
    "    n_normal = normal_rmses.shape[0]\n",
    "    n_anomaly = anomaly_rmses.shape[0]\n",
    "    roc_auc = 0\n",
    "    for normal_rmse in normal_rmses:\n",
    "        for anomaly_rmse in anomaly_rmses:\n",
    "            if normal_rmse > anomaly_rmse:\n",
    "                roc_auc += 1\n",
    "    roc_auc /= (n_normal * n_anomaly)\n",
    "    return roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
