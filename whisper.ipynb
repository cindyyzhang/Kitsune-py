{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = \"results/arp_all_vec.csv\"\n",
    "data_list = pd.read_csv(dataset).to_numpy()\n",
    "data_list = np.array([np.array([float(x) for x in row[0].split(\" \")]) for row in data_list])\n",
    "print(\"Dataset size: \", data_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "train_encoding = 200000 # number of packets to train packet feature encoding on\n",
    "train_clusters = 500000 # number of packets to train clustering module on\n",
    "W_seg = 50 # framing length \n",
    "C = 10 # adjustment in log transform\n",
    "num_clusters = 50 # number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature encodings using PCA\n",
    "train_encoding_data = data_list[:train_encoding]\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(train_encoding_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the remaining data using the learned PCA\n",
    "remaining_data = data_list[train_encoding:]\n",
    "embedded_data = pca.transform(remaining_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide packet stream into frames\n",
    "n_packets = embedded_data.shape[0]\n",
    "n_frames = n_packets // W_seg\n",
    "print(\"Number of packets: \", n_packets)\n",
    "print(\"Number of frames: \", n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DFT on each frame and calculate the modulus\n",
    "modulus_dft = []\n",
    "for i in range(n_frames):\n",
    "    frame = embedded_data[i*W_seg:(i+1)*W_seg]\n",
    "    dft_output = np.fft.fft(frame)\n",
    "    modulus_output = np.abs(dft_output)\n",
    "    modulus_dft.append(modulus_output)\n",
    "modulus_dft = np.array(modulus_dft)\n",
    "\n",
    "print(\"Modulus DFT shape: \", modulus_dft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transform to modulus of DFT outputs\n",
    "log_modulus_dft = np.log2(modulus_dft + np.ones(modulus_dft.shape))/C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten frequency domain features\n",
    "train_ind = train_clusters // W_seg\n",
    "train_data = log_modulus_dft[:train_ind,:,].reshape(train_ind, -1)\n",
    "print(train_data.shape)\n",
    "test_data = log_modulus_dft[train_ind:,:,].reshape(n_frames - train_ind, -1)\n",
    "print(test_data.shape)\n",
    "all_data = log_modulus_dft.reshape(n_frames, -1)\n",
    "print(all_data.shape)\n",
    "\n",
    "# Fit KMeans clustering model on training data\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "kmeans.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_distances(dataset, kmeans):\n",
    "    # Calculate the L2 distance between each data point and its closest cluster center\n",
    "    closest_cluster_centers = kmeans.cluster_centers_[kmeans.predict(dataset)]\n",
    "    l2_distances = np.linalg.norm(dataset - closest_cluster_centers, axis=1)\n",
    "    return l2_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of l2 distances\n",
    "plt.hist(get_l2_distances(all_data, kmeans), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from frames back to packets\n",
    "num_data_pts = data_list.shape[0]\n",
    "l2_distances = get_l2_distances(all_data, kmeans)\n",
    "l2_dist_packet = np.append(np.zeros(train_encoding),np.repeat(l2_distances, W_seg))\n",
    "print(\"L2 distances shape: \", l2_dist_packet.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packet labels for plotting\n",
    "labels = \"ARP_MitM_labels.csv\"\n",
    "with open(labels, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels_list = list(reader)\n",
    "labels_list = [int(sublist[-1]) for sublist in labels_list]\n",
    "labels_list = np.array(labels_list)\n",
    "print(\"Labels shape: \", labels_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_dist = []\n",
    "normal_dist = []\n",
    "anomaly_indices = []\n",
    "for i in range(l2_dist_packet.shape[0]):\n",
    "    if labels_list[i] == 1:\n",
    "        anomaly_dist.append(l2_dist_packet[i])\n",
    "        anomaly_indices.append(i)\n",
    "    elif i > train_encoding:\n",
    "        normal_dist.append(l2_dist_packet[i])\n",
    "\n",
    "normal_dist = np.array(normal_dist)\n",
    "anomaly_dist = np.array(anomaly_dist)\n",
    "anomaly_indices = np.array(anomaly_indices)\n",
    "normal_indices = np.setdiff1d(range(train_encoding, num_data_pts),anomaly_indices, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot L2 distances, seperate by benign/malicious\n",
    "plt.figure(figsize=(11,5))\n",
    "x_max = 4.5\n",
    "plt.xlim(0, x_max)\n",
    "bin_list = [x_max/200.0 * i for i in range(201)]\n",
    "    \n",
    "n, bins, patches = plt.hist(anomaly_dist, bins=bin_list, facecolor='g', label=\"L2 distances of malicious packets\", log=True)\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "for c, p in zip(bin_centers, patches):\n",
    "    plt.setp(p, color='#a32632', alpha=0.7)\n",
    "\n",
    "n, bins, patches = plt.hist(0.9*normal_dist, bins=bin_list, facecolor='g', label=\"L2 distances of benign packets\", log=True)\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "for c, p in zip(bin_centers, patches):\n",
    "    plt.setp(p, color='#1f9156', alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"L2 distance from nearest cluster center\", fontsize=15)\n",
    "plt.ylabel(\"Frequency\", fontsize=15)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "ax = plt.gca()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(reversed(handles), reversed(labels), loc=\"upper right\", fontsize=16)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC of ROC curve\n",
    "def get_roc_auc(normal_rmses, anomaly_rmses, sample_size=30000):\n",
    "    normal_rmses = np.random.choice(normal_rmses, sample_size)\n",
    "    anomaly_rmses = np.random.choice(anomaly_rmses, sample_size)\n",
    "    roc_auc = 0\n",
    "    for normal_rmse in normal_rmses:\n",
    "        for anomaly_rmse in anomaly_rmses:\n",
    "            if anomaly_rmse > normal_rmse:\n",
    "                roc_auc += 1\n",
    "    roc_auc /= (sample_size * sample_size)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_roc_auc(normal_dist, anomaly_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions seperated by false negative/true negative/true positive\n",
    "\n",
    "def get_column(i):\n",
    "    return [float(row[i]) for row in data_list]\n",
    "\n",
    "def get_feature(i):\n",
    "    col_i = all_data[:,i]\n",
    "    expand_i = np.append(np.zeros(train_encoding),np.repeat(col_i, W_seg))\n",
    "    return list(expand_i)\n",
    "\n",
    "threshold = 1\n",
    "cols = [1, 2, 3] \n",
    "for col in cols:\n",
    "    column = get_feature(col)\n",
    "    column_less = []\n",
    "    column_greater = []\n",
    "    column_normal = []\n",
    "    for i in range(len(normal_indices)):\n",
    "        try:\n",
    "            column_normal.append(column[normal_indices[i] - 1])\n",
    "        except:\n",
    "            break \n",
    "    for i in range(anomaly_indices.shape[0]):\n",
    "        if anomaly_dist[i] < threshold:\n",
    "            column_less.append(column[anomaly_indices[i] - 1])\n",
    "        else:\n",
    "            column_greater.append(column[anomaly_indices[i] - 1])\n",
    "\n",
    "    max_i = max(np.amax(column_less), np.amax(column_greater))\n",
    "    min_i = min(np.amin(column_less), np.amin(column_greater))\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.xlim(min_i, max_i)\n",
    "\n",
    "    bin_list = [min_i + (max_i - min_i)/200.0 * i for i in range(201)]\n",
    "\n",
    "    n, bins, patches = plt.hist(column_greater, bins=bin_list, facecolor='g', label=\"true positive\", log=True)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, color='#a32632', alpha=0.6)\n",
    "\n",
    "    n, bins, patches = plt.hist(column_normal, bins=bin_list, facecolor='g', label=\"true negative\", log=True)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, color='green', alpha=0.4)\n",
    "\n",
    "    n, bins, patches = plt.hist(column_less, bins=bin_list, facecolor='g', label=\"false negative\", log=True)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, color='#328da8', alpha=0.8)\n",
    "\n",
    "    plt.xlabel(f\"Frequency Domain Feature {col}\", fontsize=15)\n",
    "    plt.ylabel(\"Frequency\", fontsize=15)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    ax = plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(reversed(handles), reversed(labels), loc=\"upper left\", fontsize=18)\n",
    "    plt.savefig(f\"results/arp_whisper_{col}.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
